{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "sns.set()\n",
    "from PIL import Image\n",
    "from matplotlib import image\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [cv2.imread(file) for file in glob.glob(\"\\GTSRB\\Training\\00000\\*.ppm\")]\n",
    "train_data_path = \".\\dataset\\Train.csv\"\n",
    "test_data_path = \".\\dataset\\Test.csv\"\n",
    "train_path = \"dataset\\Train\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[18,5]);\n",
    "plt.hist(train_data[\"ClassId\"], bins=43, );\n",
    "plt.ylabel(\"Number of tarining examples\");\n",
    "plt.xlabel(\"Classes\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Roi.X1</th>\n",
       "      <th>Roi.Y1</th>\n",
       "      <th>Roi.X2</th>\n",
       "      <th>Roi.Y2</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.835880</td>\n",
       "      <td>50.328930</td>\n",
       "      <td>5.999515</td>\n",
       "      <td>5.962381</td>\n",
       "      <td>45.197302</td>\n",
       "      <td>44.728379</td>\n",
       "      <td>15.788390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.306933</td>\n",
       "      <td>23.115423</td>\n",
       "      <td>1.475493</td>\n",
       "      <td>1.385440</td>\n",
       "      <td>23.060157</td>\n",
       "      <td>21.971145</td>\n",
       "      <td>12.013238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>243.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Width        Height        Roi.X1        Roi.Y1        Roi.X2  \\\n",
       "count  39209.000000  39209.000000  39209.000000  39209.000000  39209.000000   \n",
       "mean      50.835880     50.328930      5.999515      5.962381     45.197302   \n",
       "std       24.306933     23.115423      1.475493      1.385440     23.060157   \n",
       "min       25.000000     25.000000      0.000000      5.000000     20.000000   \n",
       "25%       35.000000     35.000000      5.000000      5.000000     29.000000   \n",
       "50%       43.000000     43.000000      6.000000      6.000000     38.000000   \n",
       "75%       58.000000     58.000000      6.000000      6.000000     53.000000   \n",
       "max      243.000000    225.000000     20.000000     20.000000    223.000000   \n",
       "\n",
       "             Roi.Y2       ClassId  \n",
       "count  39209.000000  39209.000000  \n",
       "mean      44.728379     15.788390  \n",
       "std       21.971145     12.013238  \n",
       "min       20.000000      0.000000  \n",
       "25%       30.000000      5.000000  \n",
       "50%       38.000000     12.000000  \n",
       "75%       52.000000     25.000000  \n",
       "max      205.000000     42.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'ClassId',\n",
       "       'Path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Roi.X1</th>\n",
       "      <th>Roi.Y1</th>\n",
       "      <th>Roi.X2</th>\n",
       "      <th>Roi.Y2</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00004.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Width  Height  Roi.X1  Roi.Y1  Roi.X2  Roi.Y2  ClassId  \\\n",
       "0     27      26       5       5      22      20       20   \n",
       "1     28      27       5       6      23      22       20   \n",
       "2     29      26       6       5      24      21       20   \n",
       "3     28      27       5       6      23      22       20   \n",
       "4     28      26       5       5      23      21       20   \n",
       "\n",
       "                             Path  \n",
       "0  Train/20/00020_00000_00000.png  \n",
       "1  Train/20/00020_00000_00001.png  \n",
       "2  Train/20/00020_00000_00002.png  \n",
       "3  Train/20/00020_00000_00003.png  \n",
       "4  Train/20/00020_00000_00004.png  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "##### 1. Load image\n",
    "##### 2. Resize image to 32* 32\n",
    "##### 3. Convert to grayscale\n",
    "##### 4. Eqilize image for better contrast\n",
    "##### 5. Convert image between range [0 and 1]\n",
    "##### 6. Convert labels to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Installed_Softwares\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "img_dimentions = (32,32)\n",
    "def pre_processing_(train_data):\n",
    "    number_of_images = train_data.shape\n",
    "    number_of_images[0]\n",
    "\n",
    "    train_images = []\n",
    "\n",
    "    for i in range(number_of_images[0]):\n",
    "        img = cv2.imread(\"dataset/\"+train_data.Path[i])\n",
    "        img = cv2.resize(img, img_dimentions) # resizing images to 32 x 32 pixels\n",
    "        img_grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert RBG images to grayscale\n",
    "        img_equilize = cv2.equalizeHist(img_grayscale) # improve contrast\n",
    "        img_scale = img_grayscale / 255.00\n",
    "\n",
    "        train_images.append(img_scale)\n",
    "    # One- hot encoding labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(train_data.ClassId)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    train_labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return np.array(train_images), train_labels\n",
    "\n",
    "train_images, train_labels = pre_processing_(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "#### 1. Flip image horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flipped images: 24329 and labels: 24329\n"
     ]
    }
   ],
   "source": [
    "#     # Classes of signs that, when flipped horizontally, should still be classified as the same class\n",
    "horizontally_flippable = np.array([11, 12, 13, 15, 17, 18, 22, 26, 30, 35])\n",
    "# Classes of signs that, when flipped vertically, should still be classified as the same class\n",
    "vertically_flippable = np.array([1, 5, 12, 15, 17])\n",
    "# Classes of signs that, when flipped horizontally and then vertically, should still be classified as the same class\n",
    "both_flippable = np.array([32, 40])\n",
    "# Classes of signs that, when flipped horizontally, would still be meaningful, but should be classified as some other class\n",
    "horizontally_cross_flippable = np.array([\n",
    "    [19, 20], \n",
    "    [33, 34], \n",
    "    [36, 37], \n",
    "    [38, 39],\n",
    "    [20, 19], \n",
    "    [34, 33], \n",
    "    [37, 36], \n",
    "    [39, 38],   \n",
    "])\n",
    "\n",
    "# flipped images \n",
    "flipped_images = []\n",
    "flipped_labels = []\n",
    "\n",
    "## TODO: Optimize for loop\n",
    "for i in range(train_images.shape[0]):\n",
    "    # for horizontally flippable images\n",
    "    if np.where(train_labels[i] == 1)[0][0] in horizontally_flippable:\n",
    "        flipped_images.append(cv2.flip(train_images[i], flipCode=1)) # horizontal flip\n",
    "        flipped_labels.append(train_labels[i])\n",
    "#     print(type(train_labels[i]))\n",
    "    \n",
    "    # for vertically flippable images\n",
    "    if np.where(train_labels[i] == 1)[0][0] in vertically_flippable:\n",
    "        flipped_images.append(cv2.flip(train_images[i], flipCode=0)) # vertical flip\n",
    "        flipped_labels.append(train_labels[i])\n",
    "    # for both flippable images\n",
    "    if np.where(train_labels[i] == 1)[0][0] in both_flippable:\n",
    "        flipped_images.append(cv2.flip(train_images[i], flipCode=-1)) # both flip\n",
    "        flipped_labels.append(train_labels[i])\n",
    "    \n",
    "    if np.where(train_labels[i] == 1)[0][0] in horizontally_cross_flippable[:, 0]:\n",
    "        flipped_images.append(cv2.flip(train_images[i], flipCode=1)) # horizontally cross flip\n",
    "        data_label = int(horizontally_cross_flippable[np.where(horizontally_cross_flippable[:,0] == np.where(train_labels[i] == 1)[0][0]) ,1])\n",
    "        flipped_labels.append(to_categorical(data_label, num_classes=43))\n",
    "print(f\"Number of flipped images: {len(flipped_images)} and labels: {len(flipped_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original images: 39209 and original labels: 39209\n",
      "Number of total images: 63538 and total labels: 63538\n"
     ]
    }
   ],
   "source": [
    "# Adding flipped images to original dataset\n",
    "print(f\"Number of original images: {len(train_images)} and original labels: {len(train_labels)}\")\n",
    "train_images = np.append(train_images, flipped_images, axis = 0)\n",
    "# flipped_labels = np.array(flipped_labels)\n",
    "train_labels = np.append(train_labels, flipped_labels, axis =0)\n",
    "print(f\"Number of total images: {len(train_images)} and total labels: {len(train_labels)}\")\n",
    "# train_images = train_images.append(flipped_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.reshape(train_images,(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train,  y_test = train_test_split(train_images, train_labels, test_size=0.30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 43)                44075     \n",
      "=================================================================\n",
      "Total params: 2,399,275\n",
      "Trainable params: 2,399,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Conv layer 1\n",
    "from tensorflow.keras import models, layers\n",
    "conv_model = models.Sequential()\n",
    "conv_model.add(layers.Conv2D(32, kernel_size=5, input_shape=(32,32,1), padding='same', activation='relu'))\n",
    "conv_model.add(layers.MaxPool2D((2,2)))\n",
    "#conv_model.add(Dropout(0.1))\n",
    "\n",
    "# Conv layer 2\n",
    "conv_model.add(layers.Conv2D(64, kernel_size=5, padding='same', activation='relu'))\n",
    "conv_model.add(layers.MaxPool2D(2,2))\n",
    "#conv_model.add(Dropout(0.2))\n",
    "\n",
    "# Conv layer 3\n",
    "conv_model.add(layers.Conv2D(128, kernel_size=5, padding='same', activation='relu'))\n",
    "conv_model.add(layers.MaxPool2D(2,2))\n",
    "#conv_model.add(Dropout(0.3))\n",
    "\n",
    "# Flatten\n",
    "conv_model.add(layers.Flatten())\n",
    "#conv_model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer 1:\n",
    "conv_model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "conv_model.add(layers.Dense(43, activation='softmax'))\n",
    "\n",
    "# Save the entire model to a HDF5 file.\n",
    "conv_model.save('./model/gtsc_model.hd5') \n",
    "\n",
    "conv_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gudek\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "44448/44476 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.7747\n",
      "Epoch 00001: saving model to .\\model\\model_checkpouint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x000002578B36FFD0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From C:\\Users\\gudek\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "44476/44476 [==============================] - 230s 5ms/sample - loss: 0.8053 - acc: 0.7748\n",
      "Epoch 2/4\n",
      "44448/44476 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9739\n",
      "Epoch 00002: saving model to .\\model\\model_checkpouint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x000002578B36FFD0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "44476/44476 [==============================] - 227s 5ms/sample - loss: 0.0883 - acc: 0.9739\n",
      "Epoch 3/4\n",
      "44448/44476 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9865\n",
      "Epoch 00003: saving model to .\\model\\model_checkpouint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x000002578B36FFD0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "44476/44476 [==============================] - 241s 5ms/sample - loss: 0.0454 - acc: 0.9864\n",
      "Epoch 4/4\n",
      "44448/44476 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9889\n",
      "Epoch 00004: saving model to .\\model\\model_checkpouint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x000002578B36FFD0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "44476/44476 [==============================] - 234s 5ms/sample - loss: 0.0375 - acc: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x257befab1d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \".\\model\\model_checkpouint.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only = True, verbose = 1)\n",
    "\n",
    "conv_model.compile(optimizer= \"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training model and saving checkpoint for each epochs\n",
    "conv_model.fit(x_train, y_train, epochs=4, callbacks = [cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 29s 2ms/sample - loss: 0.0795 - acc: 0.9772\n",
      "\n",
      "Accuracy: 97.71797060966492 and Loss : 0.07954639483432134 \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = conv_model.evaluate(x_test, y_test)\n",
    "print(f\"\\nAccuracy: {test_accuracy*100} and Loss : {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot images and labels\n",
    "# actual_pred = []\n",
    "# n = 10\n",
    "# prediction = conv_model.predict(x_test[:n])\n",
    "\n",
    "# for i in range(n):\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(plt.imshow(np.reshape(x_test[i], (x_test[i].shape[0],x_test[i].shape[1]))))\n",
    "#     plt.xlabel(\"Actual label\" + y_test[i])\n",
    "#     plt.title(\"Predicted label\" + actual_pred.append(np.argmax(prediction[i])))\n",
    "    \n",
    "# for i in range(n):\n",
    "#     print(actual_pred[i], np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is 8209-4EE9\n",
      "\n",
      " Directory of D:\\Work\\Analytics\\ml\\Project\\Traffic_sign_classification_with_CNN\\german traffic signs\\model\n",
      "\n",
      "2020-03-07  03:08 PM    <DIR>          .\n",
      "2020-03-07  03:08 PM    <DIR>          ..\n",
      "2020-03-07  03:08 PM               101 checkpoint\n",
      "2020-03-07  03:08 PM         9,606,301 model_checkpouint.ckpt.data-00000-of-00001\n",
      "2020-03-07  03:08 PM             1,347 model_checkpouint.ckpt.index\n",
      "               3 File(s)      9,607,749 bytes\n",
      "               2 Dir(s)  16,677,326,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "conv_model.save('./model/gtsc_model.hd5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
